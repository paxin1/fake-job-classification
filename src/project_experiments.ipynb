{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFhYXS3fO2Ut",
    "outputId": "86d9c580-c587-4a86-c3cc-1d05929842be"
   },
   "outputs": [],
   "source": [
    "#ignore unless on colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xa2zdZT2u2Q",
    "outputId": "8194b5b2-95d7-4c59-8d78-3d965c7113db"
   },
   "outputs": [],
   "source": [
    "#ignore unless on colab\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "!pip install contractions\n",
    "!pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05BXgFEquxaq"
   },
   "outputs": [],
   "source": [
    "#main imports\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import contractions\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-phgacnz2mr"
   },
   "source": [
    "# Text Parsing\n",
    "\n",
    "This section parses the initial dataset and applies data preprocessing techniques including stemming, stopword removal, substituting contractions, and tokenization. We then choose specific features to focus on in the dataset, extract them, and convert them to Bag of Words and Tfidf feature vectorizations. The dataset is also duplicated and augumented using SMOTE to account for the imbalance fake and real job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKy-aEOBu7Vw"
   },
   "outputs": [],
   "source": [
    "#set up tools and variables to properly process and store raw text\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "documents = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5a2tjox_baz"
   },
   "source": [
    "### Using Pandas Dataframe (should be faster with larger dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6ARNziO2oHm",
    "outputId": "88636a28-6396-497e-8c9d-95dbfdff0189"
   },
   "outputs": [],
   "source": [
    "#read and process job posting dataset into pandas dataframe\n",
    "\n",
    "print('\\nLoading the file: \\n', 'input/fake_job_postings.csv')\n",
    "df = pd.read_csv('input/fake_job_postings.csv')\n",
    "print('Loaded.')\n",
    "\n",
    "Y = df[\"fraudulent\"]\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "# fill in null\n",
    "for col in [\"title\", \"company_profile\", \"description\", \"requirements\"]:\n",
    "  df[col] = df[col].fillna(\"\")\n",
    "  print(\"stemming\", col)\n",
    "    \n",
    "  #apply text processing to each column\n",
    "  df['stem_'+ col] = df.apply(lambda row: [ps.stem(word.lower()) for word in tokenizer.tokenize(contractions.fix(row[col])) if not word.lower() in stop_words], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "bpngar7__Ytp",
    "outputId": "c3bf1dc1-813a-4b1d-c43e-5595ed35675b"
   },
   "outputs": [],
   "source": [
    "#check columns for features\n",
    "\n",
    "df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbYlXL_MEJmi",
    "outputId": "ea227cc0-7e32-4c95-c42f-5abe93397a51"
   },
   "outputs": [],
   "source": [
    "# combining stemmed tokens into main dataset\n",
    "df['documents'] = df['stem_title'] + df['stem_company_profile'] + df['stem_description'] + df['stem_requirements']\n",
    "df['documents'] = df['documents'].str.join(' ')\n",
    "df['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eso2agcjE7af",
    "outputId": "e1b23cf3-b85e-4fff-c78c-b36498e09947"
   },
   "outputs": [],
   "source": [
    "#vectorize word tokens for use in model\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=0.01, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(df['documents']) #vectorize feature dataset\n",
    "print('Feature data shape: ', X.shape, '\\nLabel data shape:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively load dataset and vectorizers from previous experimentation\n",
    "\n",
    "df = pd.read_pickle('../generated/dataframe.pkl')\n",
    "vectorizer = pickle.load(open('../generated/vectorizer.pkl', 'rb'))\n",
    "Tfidf_vect = pickle.load(open('../generated/Tfidf_vect.pkl', 'rb'))\n",
    "X = pickle.load(open('../generated/X.pkl', 'rb'))\n",
    "Y = df[\"fraudulent\"]\n",
    "print('Feature data shape: ', X.shape, '\\nLabel data shape:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use SMOTE to augument fake job class data for balanced dataset\n",
    "\n",
    "sm = SMOTE(random_state=0)\n",
    "X_res, Y_res = sm.fit_resample(X,Y)\n",
    "print('Resampled data shape: ', X_res.shape, Y_res.shape)\n",
    "print('Resampled data class balance: %s' % Counter(Y_res))\n",
    "\n",
    "#split into training and testing datasets for later\n",
    "X_res_train, X_res_test, Y_res_train, Y_res_test = train_test_split(X_res, Y_res, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tfidf vectorizer to convert raw dataset into tfidf feature model and associated labels\n",
    "#adapted from https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['documents'], Y, test_size=0.5)\n",
    "\n",
    "#encode labels\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Tfidf = Encoder.fit_transform(Y_train)\n",
    "Test_Y_Tfidf = Encoder.fit_transform(Y_test)\n",
    "\n",
    "#convert dataset to tfidf feature representation\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(df['documents'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split base dataset into training and testing data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store time consuming cells in pickle files to load later\n",
    "\n",
    "df.to_pickle('../generated/dataframe.pkl')\n",
    "pickle.dump(vectorizer, open('../generated/vectorizer.pkl', 'wb'))\n",
    "pickle.dump(X, open('../generated/X.pkl', 'wb'))\n",
    "pickle.dump(Tfidf_vect, open('../generated/Tfidf_vect.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language translation data augumentation\n",
    "\n",
    "##### Unused\n",
    "\n",
    "ends up taking too long, likely because library uses outside server that rate limits requests\n",
    "\n",
    "originally meant to raw data to other language and back as a method of generating \"new\" data to augument dataset as solution for imbalance in class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts\n",
    "import translators.server as tss\n",
    "import random\n",
    "\n",
    "language_list = ['cy', 'zh', 'ru', 'fr', 'ja', 'es', 'it', 'de', 'ko', 'el', 'ar']\n",
    "\n",
    "#extract feature relevant dataframe for fraudulent job postings\n",
    "fake_df = df[df.fraudulent == 1]\n",
    "fake_df['tr_documents'] = fake_df['title'] + fake_df['company_profile'] + fake_df['description'] + fake_df['requirements'] \n",
    "fake_df = fake_df['tr_documents'].str.join('')\n",
    "\n",
    "for i in range(len(fake_df['tr_documents'])):\n",
    "    language = random.choice(language_list)\n",
    "    entry = fake_df['tr_documents'][i]\n",
    "    \n",
    "    #translate to random different language and back to generate similar entry\n",
    "    entry = tss.google(entry, from_language='en', to_language=language)\n",
    "    entry = tss.google(entry, from_language=language, to_language='en')\n",
    "    entry = ' '.join([ps.stem(word.lower()) for word in tokenizer.tokenize(contractions.fix(entry)) if not word.lower() in stop_words])\n",
    "    df['tr_documents'][i] = entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Data analysis to attempt to find important features that strongly correlate with fraudulency.\n",
    "\n",
    "For boolean features, we generated a confusion matrix to find correlation with the overall dataset and each class subset.\n",
    "\n",
    "For non-boolean features, we found the top 10 most commonly occuring values associated with fraudulency, then generated a bar plot of each feature value's percent fraudulency to compare.\n",
    "\n",
    "A function is also created to uniformly test each generated model for accuracy according to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset has large amounts of null values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get most common fraudulent values and percentage of original dataframe for selected features\n",
    "test_df = df[['location', 'department', 'salary_range', 'employment_type', 'required_experience', 'industry', 'fraudulent']]\n",
    "feature_fraud_cor = {} #dict with feature names as keys and two-tuple values of (most common fraudulent values, percentage of original dataset)\n",
    "fraud_test_df = test_df[test_df.fraudulent == 1] #get fraudulent subset of original dataframe\n",
    "for col in ['location', 'department', 'salary_range', 'employment_type', 'required_experience', 'industry']:\n",
    "    n_largest_fraud = fraud_test_df[col].value_counts().nlargest(10) #top 10 most common feature values in fraudulent subset\n",
    "    n_largest_df = test_df[test_df[col].isin(n_largest_fraud.index)] #number of occurences of each feature value in original df\n",
    "    feature_fraud_cor[col] = (n_largest_fraud, n_largest_fraud.div(n_largest_df[col].value_counts()).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['location'][0]) #print most commonly occuring fraudulent feature values\n",
    "feature_fraud_cor['location'][1].plot.bar(xlabel='location') #generate bar plot of percentage of original dataset for each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['department'][0])\n",
    "feature_fraud_cor['department'][1].plot.bar(xlabel='department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['salary_range'][0])\n",
    "feature_fraud_cor['salary_range'][1].plot.bar(xlabel='salary_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['employment_type'][0])\n",
    "feature_fraud_cor['employment_type'][1].plot.bar(xlabel='employment_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['required_experience'][0])\n",
    "feature_fraud_cor['required_experience'][1].plot.bar(xlabel='required_experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_fraud_cor['industry'][0])\n",
    "feature_fraud_cor['industry'][1].plot.bar(xlabel='industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt to correlate boolean features with fraudulency to see if there is any pattern\n",
    "\n",
    "boolean_df = df[['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']] #subset of boolean features\n",
    "real_df = boolean_df[boolean_df.fraudulent == 0] #boolean features on real jobs\n",
    "fake_df = boolean_df[boolean_df.fraudulent != 0] #boolean features on fake jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr() #correlation matrix of all boolean features and fraudulency label\n",
    "sns.heatmap(corr)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra row added to prevent zero division since all fraudulency entries are 0\n",
    "fake_nan_row = pd.Series({'telecommuting':1, 'has_company_logo':1, 'has_questions':1, 'fraudulent':1})\n",
    "real_df = real_df.append(fake_nan_row, ignore_index=True)\n",
    "\n",
    "real_corr = real_df.corr() #correlation matrix of boolean features and real jobs\n",
    "sns.heatmap(real_corr)\n",
    "print(real_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra row added to prevent zero division since all fraudulency entries are 1\n",
    "real_nan_row = pd.Series({'telecommuting':0, 'has_company_logo':0, 'has_questions':0, 'fraudulent':0})\n",
    "fake_df = fake_df.append(real_nan_row, ignore_index=True)\n",
    "\n",
    "fake_corr = fake_df.corr() #correlation matrix of boolean features and fake jobs\n",
    "sns.heatmap(fake_corr)\n",
    "print(fake_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to output model score according to various metrics\n",
    "#takes a model and train test split of data to run model predictions on\n",
    "#returns model predictions and scoring on each metric as dicts\n",
    "\n",
    "def model_score(model, X_train, X_test, Y_train, Y_test):\n",
    "    training_predictions = model.predict(X_train) #evaluate training accuracy\n",
    "    \n",
    "    train_accuracy = metrics.accuracy_score(Y_train, training_predictions)\n",
    "    print('Training accuracy: %.2f' % train_accuracy) \n",
    "\n",
    "    test_predictions = model.predict(X_test) #evaluate testing accuracy\n",
    "\n",
    "    test_accuracy = metrics.accuracy_score(Y_test, test_predictions)\n",
    "    print('Testing accuracy: %.2f' % test_accuracy)\n",
    "\n",
    "    test_predict_proba = model.predict_proba(X_test)[:,1] #evaluate testing probabilities\n",
    "    \n",
    "    test_auc_score = metrics.roc_auc_score(Y_test, test_predict_proba)\n",
    "    print('AUC value: %.2f' % test_auc_score)\n",
    "\n",
    "    bal_score = metrics.balanced_accuracy_score(Y_test, test_predictions) #evaluate balanced score (average of recall scores)\n",
    "    print('Balanced score: %.2f' % bal_score)\n",
    "\n",
    "    precision = metrics.precision_score(Y_test, test_predictions) #evaluate precision score\n",
    "    print('Precision: %.2f' % precision)\n",
    "    \n",
    "    k_predictions = model.predict(X_test[:100]) #evaluate precision score on 100 entries\n",
    "    k_precision = metrics.precision_score(Y_test[:100], k_predictions)\n",
    "    print('Precision over 100 entries: %.2f' % k_precision)\n",
    "    \n",
    "    recall = metrics.recall_score(Y_test, test_predictions) #evaluate recall score\n",
    "    print('Recall: %.2f' % recall)\n",
    "    \n",
    "    f1 = metrics.f1_score(Y_test, test_predictions) #evaluate f1 score\n",
    "    print('F1: %.2f' % f1)\n",
    "    \n",
    "    #save predictions and scores to return\n",
    "    predictions = {\n",
    "        'training': training_predictions,\n",
    "        'testing:': test_predictions,\n",
    "        'testing_proba': test_predict_proba,\n",
    "        'k': k_predictions\n",
    "    }\n",
    "    \n",
    "    scores = {\n",
    "        'training': train_accuracy,\n",
    "        'testing': test_accuracy,\n",
    "        'auc': test_auc_score,\n",
    "        'balanced': bal_score,\n",
    "        'precision': precision,\n",
    "        'k_precision': k_precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return predictions, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OWQmMk80yY3"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This section focuses on the baseline logistic regression model, experimenting with hyperparameter tuning, different feature representations, and imbalanced vs balanced dataset to compare each adjustment's effectiveness on model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "This section tunes hyperparameters for logistic regression model using grid search on created hyperparameter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish hyperparameter matrix to be search through\n",
    "\n",
    "c_space = np.logspace(-1, 3, 5).tolist()\n",
    "solvers = ['lbfgs','newton-cg','liblinear','sag','saga']\n",
    "param_grid = {'C': c_space, 'solver': solvers}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTFdgcXDH6mZ"
   },
   "outputs": [],
   "source": [
    "#run gridsearchCV to find best combination of hyperparameters over base logistic regression model\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.9)\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=True, max_iter=10000)\n",
    "log_grid = GridSearchCV(logreg, param_grid=param_grid, cv=3)\n",
    "log_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract optimal hyperparameter combination\n",
    "\n",
    "logreg_best_c, logreg_best_solver = log_grid.best_params_.values()\n",
    "print('c:', logreg_best_c, '\\nsolver:', logreg_best_solver, '\\nscore:', log_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already tuned\n",
    "\n",
    "Tuned hyperparameters previously, initializing to optimized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best_c = 0.1\n",
    "logreg_best_solver = 'liblinear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train multiple logistic regression models using different combinations of various features. We compare model accuracy while adjusting optimal hyperparameters, usage of BOW vs tfidf feature models, and imbalanced vs augumented balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regenerate tfidf feature representation and BOW dataset\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['documents'], Y, test_size=0.3)\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Tfidf = Encoder.fit_transform(Y_train)\n",
    "Test_Y_Tfidf = Encoder.fit_transform(Y_test)\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(df['documents'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression model using optimized hyperparameters\n",
    "\n",
    "logreg_model_tuned = linear_model.LogisticRegression(C=logreg_best_c, penalty='l2', solver=logreg_best_solver, fit_intercept=True, max_iter=10000)\n",
    "logreg_model_tuned.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train base logistic regression model\n",
    "\n",
    "logreg_model = linear_model.LogisticRegression(penalty='l2', fit_intercept=True, max_iter=10000)\n",
    "logreg_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression model using tfidf feature set and optimized hyperparameters\n",
    "\n",
    "logreg_model_tuned_tfidf = linear_model.LogisticRegression(C=logreg_best_c, penalty='l2', solver=logreg_best_solver, fit_intercept=True, max_iter=10000)\n",
    "logreg_model_tuned_tfidf.fit(Train_X_Tfidf, Train_Y_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train base logistic regression model using tfidf feature set\n",
    "\n",
    "logreg_model_tfidf = linear_model.LogisticRegression(penalty='l2', fit_intercept=True, max_iter=10000)\n",
    "logreg_model_tfidf.fit(Train_X_Tfidf, Train_Y_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression model using augumented dataset and optimized hyperparameters\n",
    "\n",
    "logreg_res_model_tuned = linear_model.LogisticRegression(C=logreg_best_c, penalty='l2', solver=logreg_best_solver, fit_intercept=True, max_iter=10000)\n",
    "logreg_res_model_tuned.fit(X_res_train, Y_res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train base logistic regression model using augumented dataset\n",
    "\n",
    "logreg_res_model = linear_model.LogisticRegression(penalty='l2', fit_intercept=True, max_iter=10000)\n",
    "logreg_res_model.fit(X_res_train, Y_res_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle\n",
    "Load previously trained models and scores if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model_tuned = pickle.load(open('../generated/logistic_model_tuned.pkl', 'rb'))\n",
    "logreg_model = pickle.load(open('../generated/logistic_model.pkl', 'rb'))\n",
    "logreg_model_tuned_tfidf = pickle.load(open('../generated/logistic_model_tuned_tfidf.pkl', 'rb'))\n",
    "logreg_model_tfidf = pickle.load(open('../generated/logistic_model_tfidf.pkl', 'rb'))\n",
    "logreg_res_model_tuned = pickle.load(open('../generated/logistic_res_model_tuned.pkl', 'rb'))\n",
    "logreg_res_model = pickle.load(open('../generated/logistic_res_model.pkl', 'rb'))\n",
    "logistic_scores = pickle.load(open('../generated/logistic_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Output scoring of each logistic regression model to compare model effectiveness. Various metrics are used for a more complete picture of how each model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XwlAEdlwLvX",
    "outputId": "d3260c6d-32a5-480a-bf00-9c742a26e805"
   },
   "outputs": [],
   "source": [
    "#output scoring of models using BOW dataset to train\n",
    "\n",
    "print('\\n\\nModels trained on original data:')\n",
    "\n",
    "#save each model to store later\n",
    "print('\\nTuned logistic regression:')\n",
    "logreg_tuned_preds, logreg_tuned_scores = model_score(logreg_model_tuned, X_train, X_test, Y_train, Y_test)\n",
    "print('\\nBase logistic regression:')\n",
    "logreg_preds, logreg_scores = model_score(logreg_model, X_train, X_test, Y_train, Y_test)\n",
    "print('\\nTuned logistic regression using tfidf:')\n",
    "logreg_tuned_tfidf_preds, logreg_tuned_tfidf_scores = model_score(logreg_model_tuned_tfidf, Train_X_Tfidf, Test_X_Tfidf, Train_Y_Tfidf, Test_Y_Tfidf)\n",
    "print('\\nBase logistic regression using tfidf:')\n",
    "logreg_tfidf_preds, logreg_tfidf_scores = model_score(logreg_model_tfidf, Train_X_Tfidf, Test_X_Tfidf, Train_Y_Tfidf, Test_Y_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output scoring on models using augumented dataset to train\n",
    "print('\\n\\nModels trained on SMOTE resampled data:')\n",
    "\n",
    "#save each model to store later\n",
    "print('\\nTuned logistic regression on resampled data:')\n",
    "logreg_res_tuned_preds_res, logreg_res_tuned_scores_res = model_score(logreg_res_model_tuned, X_res_train, X_res_test, Y_res_train, Y_res_test)\n",
    "print('\\nBase logistic regression on resampled data:')\n",
    "logreg_res_preds_res, logreg_res_scores_res = model_score(logreg_res_model, X_res_train, X_res_test, Y_res_train, Y_res_test)\n",
    "print('\\nTuned logistic regression on original data:')\n",
    "logreg_res_tuned_preds, logreg_res_tuned_scores = model_score(logreg_res_model_tuned, X_train, X_test, Y_train, Y_test)\n",
    "print('\\nBase logistic regression on original data:')\n",
    "logreg_res_preds, logreg_res_scores = model_score(logreg_res_model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scores for each model to dict\n",
    "\n",
    "logistic_scores = {\n",
    "    'logreg_tuned': logreg_tuned_scores,\n",
    "    'logreg': logreg_scores,\n",
    "    'logreg_tuned_tfidf': logreg_tuned_tfidf_scores,\n",
    "    'logreg_tfidf': logreg_tfidf_scores,\n",
    "    'logreg_res_tuned_res': logreg_res_tuned_scores_res,\n",
    "    'logreg_res_preds_res': logreg_res_scores_res,\n",
    "    'logreg_res_tuned': logreg_res_tuned_scores,\n",
    "    'logreg_res': logreg_res_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump models and scoring to pickle file to load later\n",
    "\n",
    "pickle.dump(logreg_model_tuned, open('../generated/logistic_model_tuned.pkl', 'wb'))\n",
    "pickle.dump(logreg_model, open('../generated/logistic_model.pkl', 'wb'))\n",
    "pickle.dump(logreg_model_tuned_tfidf, open('../generated/logistic_model_tuned_tfidf.pkl', 'wb'))\n",
    "pickle.dump(logreg_model_tfidf, open('../generated/logistic_model_tfidf.pkl', 'wb'))\n",
    "pickle.dump(logreg_res_model_tuned, open('../generated/logistic_res_model_tuned.pkl', 'wb'))\n",
    "pickle.dump(logreg_res_model, open('../generated/logistic_res_model.pkl', 'wb'))\n",
    "pickle.dump(logistic_scores, open('../generated/logistic_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJs90G3C2LQ2"
   },
   "source": [
    "## Top 5 pos/neg terms\n",
    "\n",
    "Extract the top 5 most impactful terms from logistic regression model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_DKqujf2ABj",
    "outputId": "5bea38c5-9cdb-4e78-e5aa-9eb52fb36bf0"
   },
   "outputs": [],
   "source": [
    "#sort logistic regression model weights\n",
    "K=5\n",
    "weights = logreg_res_model_tuned.coef_[0]\n",
    "terms = sorted(vectorizer.vocabulary_.keys())\n",
    "sorted_weights_terms = np.array(sorted((weights[i], terms[i]) for i in range(len(weights))))\n",
    "\n",
    "#get top 5 most postive and negative terms and weights\n",
    "topK_pos_terms = sorted_weights_terms[-1:-1-K:-1, 1]\n",
    "topK_pos_weights = sorted_weights_terms[-1:-1-K:-1, 0]\n",
    "topK_neg_terms = sorted_weights_terms[:K, 1]\n",
    "topK_neg_weights = sorted_weights_terms[:K, 0]\n",
    "\n",
    "#print weights to 2 decimal places\n",
    "print(f'\\nThe {K} *most positive* weights')\n",
    "for i in range(K):\n",
    "    print(f\"{i+1}: {topK_pos_terms[i]} \\t  {round(float(topK_pos_weights[i]), 2)}\")\n",
    "\n",
    "print(f'\\nThe {K} *most negative* weights')\n",
    "for i in range(K):\n",
    "    print(f\"{i+1}: {topK_neg_terms[i]} \\t  {round(float(topK_neg_weights[i]), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tesONH_ziV4l"
   },
   "source": [
    "# SVM \n",
    "\n",
    "This sections focuses on the support vector machine model, where we experiment with hyperparameter tuning, BOW and tfidf feature representations, and imbalanced vs balanced dataset to compare model accuracy with different adjustments.\n",
    "\n",
    "Base models and tfidf adapted from https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdhHcYxvSH9d"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43mYZPH3_-TO"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Tune hyperparameters on all combinations of user generated hyperparameter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFmF0TjC_X-8"
   },
   "outputs": [],
   "source": [
    "#take subset of base dataset to reduce runtime\n",
    "\n",
    "X_train_tuning, X_test_tuning, Y_train_tuning, Y_test_tuning = train_test_split(df['documents'], Y, test_size=0.7)\n",
    "X_train_tuning, X_test_tuning, Y_train_tuning, Y_test_tuning = train_test_split(X_train_tuning, Y_train_tuning, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcu5EJH_AFD4"
   },
   "outputs": [],
   "source": [
    "#convert smaller dataset to tfidf feature representation\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['documents'])\n",
    "Train_X_Tfidf_Tuning = Tfidf_vect.transform(X_train_tuning)\n",
    "Test_X_Tfidf_Tuning = Tfidf_vect.transform(X_test_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWOSM9MBAHF3"
   },
   "outputs": [],
   "source": [
    "#generate hyperparameter matrix\n",
    "kernels=['linear', 'poly', 'rbf', 'sigmoid']\n",
    "c_range = np.logspace(-2, 3, 6).tolist()\n",
    "gamma_range = np.logspace(-3, 2, 6).tolist()\n",
    "param_grid = dict(C=c_range, kernel=kernels, gamma=gamma_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "ZPSIdKi-AI3l",
    "outputId": "d0189789-2695-4a06-81c3-18a3e349b3d4"
   },
   "outputs": [],
   "source": [
    "#use grid search to score all combinations of hyperparameters\n",
    "svm_grid = GridSearchCV(svm.SVC(), param_grid=param_grid, refit=True)#, verbose=3)\n",
    "svm_grid.fit(Train_X_Tfidf_Tuning, Y_train_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPPjUNFIANBD"
   },
   "outputs": [],
   "source": [
    "#extract best hyperparameters from grid search\n",
    "svm_best_c, svm_best_gamma, svm_best_kernel = svm_grid.best_params_.values()\n",
    "print('c:', svm_best_c, '\\ngamma:', svm_best_gamma, '\\nkernel:', svm_best_kernel, '\\nscore:', svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already tuned\n",
    "\n",
    "Tuned hyperparameters previously, initializing to optimized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZQE6xZ5AvTV"
   },
   "outputs": [],
   "source": [
    "svm_best_c = 100.0\n",
    "svm_best_gamma = 0.01\n",
    "svm_best_kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeJViW5SR1EY"
   },
   "source": [
    "## Training\n",
    "\n",
    "Train multiple support vector machines using various combinations of optimized hyperparameters, tfidf and BOW feature sets, and balanced dataset to see most effective adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regenerate tfidf and BOW feature sets if needed\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['documents'], Y, test_size=0.3)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Tfidf = Encoder.fit_transform(Y_train)\n",
    "Test_Y_Tfidf = Encoder.fit_transform(Y_test)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['documents'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train base SVM on BOW feature set\n",
    "\n",
    "SVM = svm.SVC(probability=True, random_state=0)\n",
    "SVM.fit(X_train, Y_train)\n",
    "\n",
    "#train SVM with optimal hyperparameters on BOW feature set\n",
    "SVM_tuned = svm.SVC(probability=True, random_state=0, C=svm_best_c, gamma=svm_best_gamma, kernel=svm_best_kernel)\n",
    "SVM_tuned.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NUr2ablUIYe"
   },
   "outputs": [],
   "source": [
    "#train base SVM on tfidf feature set\n",
    "SVM_tfidf = svm.SVC(probability=True, random_state=0)\n",
    "SVM_tfidf.fit(Train_X_Tfidf, Train_Y_Tfidf)\n",
    "\n",
    "#train SVM with optimal hyperparameters on tfidf feature set\n",
    "SVM_tuned_tfidf = svm.SVC(probability=True, random_state=0, C=svm_best_c, gamma=svm_best_gamma, kernel=svm_best_kernel)\n",
    "SVM_tuned_tfidf.fit(Train_X_Tfidf, Train_Y_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train base SVM using augumented dataset\n",
    "SVM_res = svm.SVC(probability=True, random_state=0)\n",
    "SVM_res.fit(X_res_train, Y_res_train)\n",
    "\n",
    "#train SVM with optimal hyperparameters using augumented dataset\n",
    "SVM_res_tuned = svm.SVC(probability=True, random_state=0, C=svm_best_c, gamma=svm_best_gamma, kernel=svm_best_kernel)\n",
    "SVM_res_tuned.fit(X_res_train, Y_res_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle\n",
    "\n",
    "Load previously trained models and scores if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_tuned = pickle.load(open('../generated/svm_model_tuned.pkl', 'rb'))\n",
    "SVM = pickle.load(open('../generated/svm_model.pkl', 'rb'))\n",
    "SVM_tuned_tfidf = pickle.load(open('../generated/svm_model_tuned_tfidf.pkl', 'rb'))\n",
    "SVM_tfidf = pickle.load(open('../generated/svm_model_tfidf.pkl', 'rb'))\n",
    "SVM_res_tuned = pickle.load(open('../generated/svm_res_model_tuned.pkl', 'rb'))\n",
    "SVM_res = pickle.load(open('../generated/svm_res_model.pkl', 'rb'))\n",
    "SVM_scores = pickle.load(open('../generated/svm_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Output scoring for each SVM model to compare model effectiveness. Various metrics are used for a more complete picture of how each model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2eeC_F6Ue2B"
   },
   "outputs": [],
   "source": [
    "#output scoring of models that used BOW dataset to train\n",
    "\n",
    "print('\\n\\nModels trained using original data')\n",
    "print('\\nBase SVM:')\n",
    "svm_scores = model_score(SVM, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print('\\nTuned SVM:')\n",
    "svm_tuned_scores = model_score(SVM_tuned, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print('\\nBase SVM using tfidf')\n",
    "svm_tfidf_scores = model_score(SVM_tfidf, Train_X_Tfidf, Test_X_Tfidf, Train_Y_Tfidf, Test_Y_Tfidf)\n",
    "\n",
    "print('\\nTuned SVM using tfidf')\n",
    "svm_tuned_tfidf_scores = model_score(SVM_tuned_tfidf, Train_X_Tfidf, Test_X_Tfidf, Train_Y_Tfidf, Test_Y_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output scoring of models that used augumented dataset to train\n",
    "\n",
    "print('\\n\\nModels trained using SMOTE resampled data')\n",
    "print('\\nBase SVM on resampled data')\n",
    "svm_res_scores_res = model_score(SVM_res, X_res_train, X_res_test, Y_res_train, Y_res_test)\n",
    "\n",
    "print('\\nTuned SVM on resampled data')\n",
    "svm_res_tuned_scores_res = model_score(SVM_res_tuned, X_res_train, X_res_test, Y_res_train, Y_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output scoring of models that used augumented dataset to train\n",
    "print('\\nBase SVM on original data')\n",
    "svm_res_scores = model_score(SVM_res, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print('\\nTuned SVM on original data')\n",
    "svm_res_tuned_scores = model_score(SVM_res_tuned, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model scores to dict\n",
    "SVM_scores = {\n",
    "    'svm_tuned': svm_tuned_scores,\n",
    "    'svm': svm_scores,\n",
    "    'svm_tuned_tfidf': svm_tuned_tfidf_scores,\n",
    "    'svm_tfidf': svm_tfidf_scores,\n",
    "    'svm_res_tuned_res': svm_res_tuned_scores_res,\n",
    "    'svm_res_preds_res': svm_res_scores_res,\n",
    "    'svm_res_tuned': svm_res_tuned_scores,\n",
    "    'svm_res': svm_res_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to pickle\n",
    "\n",
    "Save SVM models and scores to pickle files to use in later experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump models and scores to pickle files\n",
    "\n",
    "pickle.dump(SVM, open('../generated/svm_model.pkl', 'wb'))\n",
    "pickle.dump(SVM_tuned, open('../generated/svm_model_tuned.pkl', 'wb'))\n",
    "pickle.dump(SVM_tfidf, open('../generated/svm_model_tfidf.pkl', 'wb'))\n",
    "pickle.dump(SVM_tuned_tfidf, open('../generated/svm_model_tuned_tfidf.pkl', 'wb'))\n",
    "pickle.dump(SVM_res, open('../generated/svm_res_model.pkl', 'wb'))\n",
    "pickle.dump(SVM_res_tuned, open('../generated/svm_res_model_tuned.pkl', 'wb'))\n",
    "pickle.dump(SVM_scores, open('../generated/svm_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble\n",
    "\n",
    "This section focuses on the stacking ensemble, an ensemble learning technique that combines predictions from multiple models according to meta-estimator that decides how to combine model predictions. We choose the best performing logistic regression and SVM models from previous experimentation, and combine it with an XGBoost classifier, using an Extra Trees classifier to find the best weighting for each of the lower level models. The ensemble is trained on the tfidf feature representation and augumented BOW representation to compare effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Selects multiple lower level classifiers to fit to an extra trees classifier, then trains the stacking model on tfidf feature representation and augumented BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize lower level classifiers to be used\n",
    "lower_classifiers = dict()\n",
    "lower_classifiers[\"logreg\"] = logreg_res_model\n",
    "lower_classifiers[\"xgboost\"] = XGBClassifier(objective='binary:logistic', eval_metric='aucpr')\n",
    "lower_classifiers[\"svm\"] = SVM_res_tuned\n",
    "\n",
    "#initialize higher level classifier\n",
    "aggregate_classifier = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regenerate tfidf feature representation if needed\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['documents'], Y, test_size=0.3)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Tfidf = Encoder.fit_transform(Y_train)\n",
    "Test_Y_Tfidf = Encoder.fit_transform(Y_test)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['documents'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Train_X_Tfidf, Train_Y_Tfidf, test_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train stacking model on tfidf feature dataset\n",
    "stacking_model_tfidf = StackingClassifier(estimators=list(lower_classifiers.items()), final_estimator=aggregate_classifier, passthrough=True, stack_method=\"predict_proba\", verbose=2)\n",
    "stacking_model_tfidf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train stacking model on augumented BOW dataset\n",
    "stacking_res_model = StackingClassifier(estimators=list(lower_classifiers.items()), final_estimator=aggregate_classifier, passthrough=True, stack_method=\"predict_proba\", verbose=2)\n",
    "stacking_res_model.fit(X_res_train, Y_res_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle\n",
    "\n",
    "Load previously trained models and scores if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stacking models and scores\n",
    "\n",
    "stacking_model_tfidf = pickle.load(open('../generated/ensemble_model_tfidf.pkl', 'rb'))\n",
    "stacking_res_model = pickle.load(open('../generated/ensemble_res_model.pkl', 'rb'))\n",
    "ensemble_scores = pickle.load(open('../generated/ensemble_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "Output scoring for each stacking model to compare model effectiveness. Various metrics are used for a more complete picture of how each model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print scores for each model\n",
    "\n",
    "print('Stacking ensemble using tfidf:')\n",
    "stacking_model_tfidf_scores = model_score(stacking_model_tfidf, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print('\\nStacking ensemble using SMOTE resampled data:')\n",
    "stacking_res_model_scores = model_score(stacking_res_model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model scores to dict\n",
    "ensemble_scores = {\n",
    "    'ensemble_tfidf': stacking_model_tfidf_scores,\n",
    "    'ensemble_res': stacking_res_model_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacking_res_model.estimators_[0])\n",
    "print(stacking_res_model.estimators_[1])\n",
    "print(stacking_res_model.estimators_[2])\n",
    "print(stacking_res_model.final_estimator_)\n",
    "print(stacking_res_model.stack_method_)\n",
    "print(stacking_res_model.estimators_[1].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to pickle\n",
    "Save stacking models and scores to pickle files to use in later experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump models and score to pickle files\n",
    "pickle.dump(stacking_model_tfidf, open('../generated/ensemble_model_tfidf.pkl', 'wb'))\n",
    "pickle.dump(stacking_res_model, open('../generated/ensemble_res_model.pkl', 'wb'))\n",
    "pickle.dump(ensemble_scores, open('../generated/ensemble_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P9H5PwEMVIq"
   },
   "source": [
    "# Bidirectional LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7PIjxpxUdUL"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq2BznDtUlRm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load previously trained model if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = keras.models.load_model('../generated/lstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82wvMau4McPO"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD8wmRlSVKzw"
   },
   "outputs": [],
   "source": [
    "df['documents']\n",
    "voc_size=5000\n",
    "sent_length = 50 # make all sentences the same length, and add white space if necessary\n",
    "\n",
    "X = df['documents'].values\n",
    "Y = list(Y)\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=voc_size, output_sequence_length=sent_length) # Layer for vectorizing our text data\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((X, Y)) # create a tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqKdPDpWFc5e",
    "outputId": "21727bed-e7a1-4c72-8826-a218113b193f"
   },
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=0.25, random_state=32) # split our dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "tf_dataset_tr = tf.data.Dataset.from_tensor_slices((X_tr,Y_tr)) # create tensorflow training dataset\n",
    "tf_dataset_tr = tf_dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "encoder.adapt(tf_dataset_tr.map(lambda text, label: text)) # adapt our encoder with our training data\n",
    "\n",
    "tf_dataset_te = tf.data.Dataset.from_tensor_slices((X_te,Y_te)) # create tensorflow testing dataset\n",
    "tf_dataset_te = tf_dataset_te.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "total = len(X_tr)\n",
    "neg = len([x for x in Y_tr if x == 0])\n",
    "pos = len([x for x in Y_tr if x == 1])\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating model\n",
    "embedding_vector_features=50\n",
    "lstm_model=Sequential() # create model\n",
    "\n",
    "lstm_model.add(encoder) # add encoder layer\n",
    "\n",
    "lstm_model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length)) # embedding layer to standardize data\n",
    "lstm_model.add(Bidirectional(LSTM(100))) # LSTM layer that processes data in both directions\n",
    "lstm_model.add(Dropout(0.3)) # prevent overfitting with dropout layer\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) \n",
    "\n",
    "lstm_model.fit(tf_dataset_tr, validation_data=tf_dataset_te,epochs=12,batch_size=BATCH_SIZE, class_weight=class_weights) # fit data to training data with 12 epochs\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance\n",
    "Y_pred = (lstm_model.predict(X_te) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(Y_te,Y_pred))\n",
    "print(classification_report(Y_te, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save('../generated/lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2OWQmMk80yY3",
    "tesONH_ziV4l"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
